---
sidebar: sidebar
permalink: use/upgrade-acc.html
keywords: astra upgrade, upgrade astra control center, how to upgrade astra control, update
summary: To upgrade Astra Control Center, you'll download the bundle and upgrade following the steps described.
---

= Upgrade Astra Control Center
:hardbreaks:
:icons: font
:imagesdir: ../media/get-started/

To upgrade Astra Control Center, download the installation bundle from the NetApp Support Site and perform a series a commands to upgrade the Astra Control Center components in your environment. You can use this procedure to upgrade Astra Control Center in internet-connected or air-gapped environments.

.What you'll need
* link:requirements.html[Before you begin upgrade, ensure your environment still meets the minimum requirements for Astra Control Center deployment].
* From your OpenShift cluster, ensure all cluster operators are in a healthy state (`available` is `true`):
+
----
oc get clusteroperators
----

* From your OpenShift cluster, ensure all API services are in a healthy state (`available` is `true`):
+
----
oc get apiservices
----
* Log out of your Astra Control Center.

.About this task
The Astra Control Center upgrade process guides you through the following high-level steps:

* <<Download the Astra Control Center bundle>>
* <<Unpack the bundle and change directory>>
* <<Add the images to your local registry>>
* <<Install the updated Astra Control Center operator>>
* <<Upgrade Astra Control Center>>
* <<Verify system status>>
* <<Upgrade third-party services>>

NOTE: Podman commands can be used in place of Docker commands if you are using Red Hatâ€™s Podman repository.

== Download the Astra Control Center bundle

. Download the Astra Control Center upgrade bundle (`astra-control-center-[version].tar.gz`) from the https://mysupport.netapp.com/site/products/all/details/astra-control-center/downloads-tab[NetApp Support Site^].
. (Optional) Use the following command to verify the signature of the bundle:
+
----
openssl dgst -sha256 -verify astra-control-center[version].pub -signature <astra-control-center[version].sig astra-control-center[version].tar.gz
----

== Unpack the bundle and change directory

. Extract the images:
+
----
tar -vxzf astra-control-center-[version].tar.gz
----

. Change to the Astra directory.
+
----
cd astra-control-center-[version]
----

== Add the images to your local registry

. Add the files in the Astra Control Center image directory to your local registry.
+
NOTE: See a sample script for the automatic loading of images below.

.. Log in to your Docker registry:
+
----
docker login [your_registry_path]
----

.. Load the images into Docker.
.. Tag the images.
.. [[substep_image_local_registry_push]]Push the images to your local registry.
+
----
export REGISTRY=[your_registry_path]
for astraImageFile in $(ls images/*.tar)
  # Load to local cache. And store the name of the loaded image trimming the 'Loaded images: '
  do astraImage=$(docker load --input ${astraImageFile} | sed 's/Loaded image(s): //')
  astraImage=$(echo ${astraImage} | sed 's!localhost/!!')
  # Tag with local image repo.
  docker tag ${astraImage} ${REGISTRY}/${astraImage}
  # Push to the local repo.
  docker push ${REGISTRY}/${astraImage}
done
----

== Install the updated Astra Control Center operator

. Edit the Astra Control Center operator deployment yaml (`astra_control_center_operator_deploy.yaml`) to refer to your local registry and secret.
+
----
vim astra_control_center_operator_deploy.yaml
----

.. If you use a registry that requires authentication, replace the default line of `imagePullSecrets: []` with the following:
+
----
imagePullSecrets:
- name: astra-registry-cred
----

.. Change `[your_registry_path]` for the `kube-rbac-proxy` image to the registry path where you pushed the images in a <<substep_image_local_registry_push,previous step>>.
.. Change `[your_registry_path]` for the `acc-operator-controller-manager` image to the registry path where you pushed the images in a <<substep_image_local_registry_push,previous step>>.

+
[subs=+quotes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    control-plane: controller-manager
  name: acc-operator-controller-manager
  namespace: netapp-acc-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: controller-manager
  template:
    metadata:
      labels:
        control-plane: controller-manager
    spec:
      containers:
      - args:
        - --secure-listen-address=0.0.0.0:8443
        - --upstream=http://127.0.0.1:8080/
        - --logtostderr=true
        - --v=10
        *image: [your_registry_path]/kube-rbac-proxy:v4.8.0*
        name: kube-rbac-proxy
        ports:
        - containerPort: 8443
          name: https
      - args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --leader-elect
        command:
        - /manager
        env:
        - name: ACCOP_LOG_LEVEL
          value: "2"
        *image: [your_registry_path]/acc-operator:[version x.y.z]*
        imagePullPolicy: IfNotPresent
      *imagePullSecrets: []*
----

. Install the updated Astra Control Center operator:
+
----
kubectl apply -f astra_control_center_operator_deploy.yaml
----
+
Sample response:
+
----
namespace/netapp-acc-operator created
customresourcedefinition.apiextensions.k8s.io/astracontrolcenters.astra.netapp.io created
role.rbac.authorization.k8s.io/acc-operator-leader-election-role created
clusterrole.rbac.authorization.k8s.io/acc-operator-manager-role created
clusterrole.rbac.authorization.k8s.io/acc-operator-metrics-reader created
clusterrole.rbac.authorization.k8s.io/acc-operator-proxy-role created
rolebinding.rbac.authorization.k8s.io/acc-operator-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/acc-operator-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/acc-operator-proxy-rolebinding created
configmap/acc-operator-manager-config created
service/acc-operator-controller-manager-metrics-service created
deployment.apps/acc-operator-controller-manager created
----

== Upgrade Astra Control Center

. Edit the Astra Control Center custom resource (CR) and change the Astra version (`astraVersion`) number to the latest:
+
----
kubectl edit acc -n [netapp-acc or custom namespace] astra
----
+
NOTE: Changing the Astra version is the only requirement for an Astra Control Center upgrade. Your registry path must match the registry path where you pushed the images in a <<substep_image_local_registry_push,previous step>>.

. Verify that the pods terminate and become available again:
+
----
watch kubectl get po -n [netapp-acc or custom namespace]
----

. Verify that the Astra status conditions indicate that the upgrade is complete and ready:
+
----
kubectl get -o yaml -n [netapp-acc or custom namespace] astracontrolcenters.astra.netapp.io astra
----
+
Response:
+
----
conditions:
  - lastTransitionTime: "2021-10-25T18:49:26Z"
    message: Astra is deployed
    reason: Complete
    status: "True"
    type: Ready
  - lastTransitionTime: "2021-10-25T18:49:26Z"
    message: Upgrading succeeded.
    reason: Complete
    status: "False"
    type: Upgrading
----

== Verify system status

. Log in to Astra Control Center.
. Verify that all your managed clusters and apps are still present and protected.

== Upgrade third-party services
The third-party services Traefik and Cert-manager are not upgraded during earlier upgrade steps. You can optionally upgrade them using the procedure described here or retain existing service versions if your system requires it. The following is the recommended Traefik and Certs-manager upgrade sequence:

. <<Set up acc-helm-repo to upgrade Traefik and Cert-manager>>
. <<Update Traefik service using acc-helm-repo>>
. <<Update the Cert-manager service>>

== Set up acc-helm-repo to upgrade Traefik and Cert-manager

. Find the `enterprise-helm-repo` that is loaded to your local Docker cache:
+
----
docker images enterprise-helm-repo
----
+
Response:
+
----
REPOSITORY             TAG         IMAGE ID       CREATED        SIZE
enterprise-helm-repo   21.10.218   7a182d6b30f3   20 hours ago   464MB
----

. Start a container using the tag from the previous step:
+
----
docker run -dp 8082:8080 enterprise-helm-repo:21.10.218
----
+
Response:
+
----
940436e67fa86d2c4559ac4987b96bb35588313c2c9ddc9cec195651963f08d8
----

. Add the Helm repo to your local host repositories:
+
----
helm repo add acc-helm-repo http://localhost:8082/
----
+
Response:
+
----
"acc-helm-repo" has been added to your repositories
----

. Save the following Python script as a file, for example, `set_previous_values.py`:
+
NOTE: This Python script creates two files that are used in later upgrade steps to retain helm values.

+
----
#!/usr/bin/env python3
import json
import os

NAMESPACE = "netapp-acc"

os.system(f"helm get values traefik -n {NAMESPACE} -o json > traefik_values.json")
os.system(f"helm get values cert-manager -n {NAMESPACE} -o json > cert_manager_values.json")

# reformat traefik values
f = open("traefik_values.json", "r")
traefik_values = {'traefik': json.load(f)}
f.close()

with open('traefik_values.json', 'w') as output_file:
    json.dump(traefik_values, output_file)

# reformat cert-manager values
f = open("cert_manager_values.json", "r")
cm_values = {'cert-manager': json.load(f)}
f.close()

cm_values['global'] = cm_values['cert-manager']['global']
del cm_values['cert-manager']['global']

with open('cert_manager_values.json', 'w') as output_file:
    json.dump(cm_values, output_file)

print('Done')
----

. Run the script:
+
----
python3.7 ./set_previous_values.py
----

== Update Traefik service using acc-helm-repo

NOTE: You must already have <<Set up acc-helm-repo to upgrade Traefik and Cert-manager,set up acc-helm-repo>> before completing the following procedure.

. If you are installing from an internet-connected environment, download the Traefik bundle using a secure, file-transfer tool, such as GNU wget:
+
----
wget http://localhost:8082/traefik-X.X.X.tgz
----

. Extract the images:
+
----
tar -vxzf traefik-X.X.X.tgz
----

. Apply the Traefik CRDs:
+
----
kubectl apply -f ./traefik/charts/traefik/crds/
----

. Find the Helm chart version to use with your upgraded Traefik:
+
----
helm search repo acc-helm-repo/traefik
----
+
Response:
+
----
NAME                                    CHART VERSION   APP VERSION DESCRIPTION
local-temp-repo/traefik                 X.X.X           X.X.X       Helm chart for Traefik Ingress controller
local-temp-repo/traefik-ingressroutes   X.X.X           X.X.X       A Helm chart for Kubernetes
----

. Upgrade your Traefik configuration:
+
----
helm upgrade --version X.X.X  --namespace netapp-acc -f traefik_values.json traefik acc-helm-repo/traefik
----
+
Response:
+
----
Release "traefik" has been upgraded. Happy Helming!
NAME: traefik
LAST DEPLOYED: Mon Oct 25 22:53:19 2021
NAMESPACE: netapp-acc
STATUS: deployed
REVISION: 2
TEST SUITE: None
----

== Update the Cert-manager service

NOTE: You must already have completed the <<Update Traefik service using acc-helm-repo,Traefik update>> and <<Set up acc-helm-repo to upgrade Traefik and Cert-manager,added acc-helm-repo in Helm>> before completing the following procedure.

. If you are installing from an internet-connected environment, download the Cert-manager bundle using a secure, file-transfer tool, such as GNU wget:
+
----
wget https://github.com/jetstack/cert-manager/releases/download/vX.X.X/cert-manager.crds.yaml
----

. Apply the Certs-manager CRDs:
+
----
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/vX.X.X/cert-manager.crds.yaml
----
+
Response:
+
----
Warning: resource customresourcedefinitions/certificaterequests.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io configured
Warning: resource customresourcedefinitions/certificates.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io configured
Warning: resource customresourcedefinitions/challenges.acme.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io configured
Warning: resource customresourcedefinitions/clusterissuers.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io configured
Warning: resource customresourcedefinitions/issuers.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io configured
Warning: resource customresourcedefinitions/orders.acme.cert-manager.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io configured
----

. Upgrade your Cert-manager configuration:
+
----
helm upgrade  --version X.X.X --namespace netapp-acc -f cert_manager_values.json cert-manager acc-helm-repo/cert-manager
----
+
Response:
+
----
Release "cert-manager" has been upgraded. Happy Helming!
NAME: cert-manager
LAST DEPLOYED: Thu Oct 28 15:26:39 2021
NAMESPACE: netapp-acc
STATUS: deployed
REVISION: 2
TEST SUITE: Non
----

. Verify that all system components upgraded successfully.
+
----
kubectl get pods -n [netapp-acc or custom]
----
+
Each pod should have a status of `Running` and `Age` that is recent. It may take several minutes before the system pods are deployed.
+
Sample response:
+
----
NAME                                         READY   STATUS    RESTARTS   AGE
acc-helm-repo-5fdfff786f-gkv6z               1/1     Running   0          4m58s
activity-649f869bf7-jn5gs                    1/1     Running   0          3m14s
asup-79846b5fdc-s9s97                        1/1     Running   0          3m10s
authentication-84c78f5cf4-qhx9t              1/1     Running   0          118s
billing-9b8496787-v8rzv                      1/1     Running   0          2m54s
bucketservice-5fb876d9d5-wkfvz               1/1     Running   0          3m26s
cloud-extension-f9f4f59c6-dz6s6              1/1     Running   0          3m
cloud-insights-service-5676b8c6d4-6q7lv      1/1     Running   0          2m52s
composite-compute-7dcc9c6d6c-lxdr6           1/1     Running   0          2m50s
composite-volume-74dbfd7577-cd42b            1/1     Running   0          3m2s
credentials-75dbf46f9d-5qm2b                 1/1     Running   0          3m32s
entitlement-6cf875cb48-gkvhp                 1/1     Running   0          3m12s
features-74fd97bb46-vss2n                    1/1     Running   0          3m6s
fluent-bit-ds-2g9jb                          1/1     Running   0          113s
fluent-bit-ds-5tg5h                          1/1     Running   0          113s
fluent-bit-ds-qfxb8                          1/1     Running   0          113s
graphql-server-7769f98b86-p4qrv              1/1     Running   0          90s
identity-566c566cd5-ntfj6                    1/1     Running   0          3m16s
influxdb2-0                                  1/1     Running   0          4m43s
krakend-5cb8d56978-44q66                     1/1     Running   0          93s
license-66cbbc6f48-27kgf                     1/1     Running   0          3m4s
login-ui-584f7fd84b-dmdrp                    1/1     Running   0          87s
loki-0                                       1/1     Running   0          4m44s
metrics-ingestion-service-6dcfddf45f-mhnvh   1/1     Running   0          3m8s
monitoring-operator-78d67b4d4-nxs6v          2/2     Running   0          116s
nats-0                                       1/1     Running   0          4m40s
nats-1                                       1/1     Running   0          4m26s
nats-2                                       1/1     Running   0          4m15s
nautilus-9b664bc55-rn9t8                     1/1     Running   0          2m56s
openapi-dc5ddfb7d-6q8vh                      1/1     Running   0          3m20s
polaris-consul-consul-5tjs7                  1/1     Running   0          4m43s
polaris-consul-consul-5wbnx                  1/1     Running   0          4m43s
polaris-consul-consul-bfvl7                  1/1     Running   0          4m43s
polaris-consul-consul-server-0               1/1     Running   0          4m43s
polaris-consul-consul-server-1               1/1     Running   0          4m43s
polaris-consul-consul-server-2               1/1     Running   0          4m43s
polaris-mongodb-0                            2/2     Running   0          4m49s
polaris-mongodb-1                            2/2     Running   0          4m22s
polaris-mongodb-2                            1/1     Running   0          4m49s
polaris-ui-6648875998-75d98                  1/1     Running   0          92s
polaris-vault-0                              1/1     Running   0          4m41s
polaris-vault-1                              1/1     Running   0          4m41s
polaris-vault-2                              1/1     Running   0          4m41s
storage-backend-metrics-69546f4fc8-m7lfj     1/1     Running   0          3m22s
storage-provider-5d46f755b-qfv89             1/1     Running   0          3m30s
support-5dc579865c-z4pwq                     1/1     Running   0          3m18s
telegraf-ds-4452f                            1/1     Running   0          113s
telegraf-ds-gnqxl                            1/1     Running   0          113s
telegraf-ds-jhw74                            1/1     Running   0          113s
telegraf-rs-gg6m4                            1/1     Running   0          113s
telemetry-service-6dcc875f98-zft26           1/1     Running   0          3m24s
tenancy-7f7f77f699-q7l6w                     1/1     Running   0          3m28s
traefik-769d846f9b-c9crt                     1/1     Running   0          83s
traefik-769d846f9b-l9n4k                     1/1     Running   0          67s
trident-svc-8649c8bfc5-pdj79                 1/1     Running   0          2m57s
vault-controller-745879f98b-49c5v            1/1     Running   0          4m51s
----
